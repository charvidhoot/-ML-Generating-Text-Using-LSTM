{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  163817\n",
      "Total Vocab:  61\n",
      "Total Patterns:  163717\n",
      "Epoch 1/20\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 3.0063Epoch 00000: loss improved from inf to 3.00623, saving model to weights-improvement-00-3.0062.hdf5\n",
      "163717/163717 [==============================] - 1985s - loss: 3.0062  \n",
      "Epoch 2/20\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 2.8231Epoch 00001: loss improved from 3.00623 to 2.82309, saving model to weights-improvement-01-2.8231.hdf5\n",
      "163717/163717 [==============================] - 1996s - loss: 2.8231  \n",
      "Epoch 3/20\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 2.7263Epoch 00002: loss improved from 2.82309 to 2.72626, saving model to weights-improvement-02-2.7263.hdf5\n",
      "163717/163717 [==============================] - 1971s - loss: 2.7263  \n",
      "Epoch 4/20\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 2.6599Epoch 00003: loss improved from 2.72626 to 2.65991, saving model to weights-improvement-03-2.6599.hdf5\n",
      "163717/163717 [==============================] - 2091s - loss: 2.6599  \n",
      "Epoch 5/20\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 2.6035Epoch 00004: loss improved from 2.65991 to 2.60349, saving model to weights-improvement-04-2.6035.hdf5\n",
      "163717/163717 [==============================] - 2038s - loss: 2.6035  \n",
      "Epoch 6/20\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 2.5461Epoch 00005: loss improved from 2.60349 to 2.54614, saving model to weights-improvement-05-2.5461.hdf5\n",
      "163717/163717 [==============================] - 1995s - loss: 2.5461  \n",
      "Epoch 7/20\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 2.4908Epoch 00006: loss improved from 2.54614 to 2.49079, saving model to weights-improvement-06-2.4908.hdf5\n",
      "163717/163717 [==============================] - 2020s - loss: 2.4908  \n",
      "Epoch 8/20\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 2.4430Epoch 00007: loss improved from 2.49079 to 2.44299, saving model to weights-improvement-07-2.4430.hdf5\n",
      "163717/163717 [==============================] - 1998s - loss: 2.4430  \n",
      "Epoch 9/20\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 2.3987Epoch 00008: loss improved from 2.44299 to 2.39875, saving model to weights-improvement-08-2.3988.hdf5\n",
      "163717/163717 [==============================] - 2019s - loss: 2.3988  \n",
      "Epoch 10/20\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 2.3580Epoch 00009: loss improved from 2.39875 to 2.35799, saving model to weights-improvement-09-2.3580.hdf5\n",
      "163717/163717 [==============================] - 2034s - loss: 2.3580  \n",
      "Epoch 11/20\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 2.3184Epoch 00010: loss improved from 2.35799 to 2.31845, saving model to weights-improvement-10-2.3184.hdf5\n",
      "163717/163717 [==============================] - 2029s - loss: 2.3184  \n",
      "Epoch 12/20\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 2.2864Epoch 00011: loss improved from 2.31845 to 2.28647, saving model to weights-improvement-11-2.2865.hdf5\n",
      "163717/163717 [==============================] - 2035s - loss: 2.2865  \n",
      "Epoch 13/20\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 2.2501Epoch 00012: loss improved from 2.28647 to 2.25007, saving model to weights-improvement-12-2.2501.hdf5\n",
      "163717/163717 [==============================] - 2038s - loss: 2.2501  \n",
      "Epoch 14/20\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 2.2217Epoch 00013: loss improved from 2.25007 to 2.22172, saving model to weights-improvement-13-2.2217.hdf5\n",
      "163717/163717 [==============================] - 2035s - loss: 2.2217  \n",
      "Epoch 15/20\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 2.1903Epoch 00014: loss improved from 2.22172 to 2.19027, saving model to weights-improvement-14-2.1903.hdf5\n",
      "163717/163717 [==============================] - 2040s - loss: 2.1903  \n",
      "Epoch 16/20\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 2.1611Epoch 00015: loss improved from 2.19027 to 2.16107, saving model to weights-improvement-15-2.1611.hdf5\n",
      "163717/163717 [==============================] - 2044s - loss: 2.1611  \n",
      "Epoch 17/20\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 2.1316Epoch 00016: loss improved from 2.16107 to 2.13157, saving model to weights-improvement-16-2.1316.hdf5\n",
      "163717/163717 [==============================] - 2036s - loss: 2.1316  \n",
      "Epoch 18/20\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 2.1063Epoch 00017: loss improved from 2.13157 to 2.10633, saving model to weights-improvement-17-2.1063.hdf5\n",
      "163717/163717 [==============================] - 2034s - loss: 2.1063  \n",
      "Epoch 19/20\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 2.0855Epoch 00018: loss improved from 2.10633 to 2.08551, saving model to weights-improvement-18-2.0855.hdf5\n",
      "163717/163717 [==============================] - 2022s - loss: 2.0855  \n",
      "Epoch 20/20\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 2.0634Epoch 00019: loss improved from 2.08551 to 2.06343, saving model to weights-improvement-19-2.0634.hdf5\n",
      "163717/163717 [==============================] - 2025s - loss: 2.0634  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12af5af98>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Small LSTM Network to Generate Text for Alice in Wonderland\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "# load ascii text and covert to lowercase\n",
    "filename = \"wonderland.txt\"\n",
    "raw_text = open(filename).read()\n",
    "raw_text = raw_text.lower()\n",
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "# summarize the loaded data\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print (\"Total Characters: \", n_chars)\n",
    "print (\"Total Vocab: \", n_vocab)\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "\tseq_in = raw_text[i:i + seq_length]\n",
    "\tseq_out = raw_text[i + seq_length]\n",
    "\tdataX.append([char_to_int[char] for char in seq_in])\n",
    "\tdataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print (\"Total Patterns: \", n_patterns)\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "# fit the model\n",
    "model.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"weights-improvement-19-2.0634.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" aid the king; and the white rabbit blew three\n",
      "blasts on the trumpet, and called out, ‘first witness! \"\n",
      "t aaded the drtphon. \n",
      "‘h tontt i said to toe toitt ’our tarlen,’ said the mock turtle. \n",
      "‘io yhu mage in an iore ’fu ’our tajd ’ shi mock turtle replied  sardin to herself  the would bale to the thrte tat in the wordd the sabbit was so tie thrter. and the whst oo ti the three the had soo thet she was soe tire tith the war soeneing the tas to tee thet she had seter her hnet the had tote the whste tat in the wordd to tee thet she had soted the was toin in the was toen it whs  and toe tan the harter whst she was toe tiree to the thrte the was toin in the was toen it whs  and toe tan the harter whst she was toe tiree to the thrte the was toin in the was toen it whs  and toe tan the harter whst she was toe tiree to the thrte the was toin in the was toen it whs  and toe tan the harter whst she was toe tiree to the thrte the was toin in the was toen it whs  and toe tan the harter whst she was toe tiree to the thrte the was toin in the was toen it whs  and toe tan the harter whst she was toe ti\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# pick a random seed\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print (\"Seed:\")\n",
    "print (\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "\tx = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "\tx = x / float(n_vocab)\n",
    "\tprediction = model.predict(x, verbose=0)\n",
    "\tindex = numpy.argmax(prediction)\n",
    "\tresult = int_to_char[index]\n",
    "\tseq_in = [int_to_char[value] for value in pattern]\n",
    "\tsys.stdout.write(result)\n",
    "\tpattern.append(index)\n",
    "\tpattern = pattern[1:len(pattern)]\n",
    "print (\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#added a comment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
